{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook E-tivity 3 CE4021 Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student name: Collin Czernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student ID: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you believe required imports are missing, please contact your moderator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below information to create a Naive Bayes SPAM filter. Test your filter using the messages in new_emails. You may add as many cells as you require to complete the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Naive Bayes Classifier to filter incoming mail for SPAM. Write code using Bayes' Rule to determine whether the messages contained in new_emails are HAM or SPAM. Compare the decisions your classifier takes with the label associated with the messages. If time permits, add the code required to allow your classifier to learn from the email messages contained in new_emails. Note that this functionality is required to be graded in the Exemplary category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_spam = ['send us your password', 'review our website', 'send your password', 'send us your account']\n",
    "previous_ham = ['Your activity report','benefits physical activity', 'the importance vows']\n",
    "new_emails = {'spam':['renew your password', 'renew your vows'], 'ham':['benefits of our account', 'the importance of physical activity']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So initial thoughts - split up the emails and look for key words. Learning: function at end that if the email was marked \"HAM\", add key phrases to \"previous ham\" list.\n",
    "\n",
    "From core resources on Bayes Theorem, $P(H|E) = \\frac{P(H) \\cdot P(E|H)}{P(E)}$\n",
    "\n",
    "Breaking this down:\n",
    "$P(H)$ is the probability a hypothesis is true (before any evidence), $P(E|H)$ is the probability of seeing the evidence that the hypothesis is true,\n",
    "$P(E)$ is the probability of seeing the evidence, $P(H|E)$ is the probability a hypothesis is true given some evidence.\n",
    "\n",
    "For every word that is in the previous_spam list, we assign a probability such that if that word appears again, it counts (see assignment for how I did this). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'renew your password'} is likely {'Spam'}\n",
      "{'renew your vows'} is likely {'Spam'}\n",
      "{'benefits of our account'} is likely {'Ham'}\n",
      "{'the importance of physical activity'} is likely {'Ham'}\n"
     ]
    }
   ],
   "source": [
    "saved_spam = [] # for the \"learning\" part\n",
    "saved_ham = [] # for the \"learning\" part\n",
    "\n",
    "def split_list(email_list): # Basically, for each new email in the list splitting it up into individual words \n",
    "    words = [] # Storage for our new words \n",
    "    for i in email_list:\n",
    "        words.extend(i.split())\n",
    "    return words\n",
    "\n",
    "spam_words = split_list(previous_spam) # getting all the words in previous spam emails\n",
    "ham_words = split_list(previous_ham) # getting all the words in previous ham emails\n",
    "\n",
    "def count_words(word_list): # Counting the words in the original spam, ham lists. This is dynamic so should allow for updating. \n",
    "    word_count = {} # Setting up dictionary to save them as like 'word': 5, meaning 'word' appeared 5 times\n",
    "    for word in word_list: # Word is starting to look like not an actual word... running loop for through word_list\n",
    "        word_count[word] = word_count.get(word, 0) + 1 # setting the word count for each word in the list and adding it to the dict\n",
    "    return word_count\n",
    "\n",
    "spam_count = count_words(spam_words) # getting all the word counts for spam emails\n",
    "ham_count = count_words(ham_words) # getting all the word counts for ham emails \n",
    "\n",
    "def word_probabilities(word_counts, total_words, smooth=1): # Need to find the probability of each word occurring in spam or ham\n",
    "    word_probs = {}\n",
    "    for word, count in word_counts.items(): # We need to loop through the items in word_count to figure out the probability of each word \n",
    "        word_probs[word] = (count + smooth) / (total_words + len(word_counts)) # Laplace smoothing of +1\n",
    "    return word_probs \n",
    "\n",
    "total_spam_words = len(spam_words) # from previous spam list, how many spam words are there\n",
    "total_ham_words = len(ham_words) # from previous ham list, how many ham words are there\n",
    "\n",
    "spam_word_probs = word_probabilities(spam_count, total_spam_words) # probability of each word in previous_spam being a spam email \n",
    "ham_word_probs = word_probabilities(ham_count, total_ham_words) # probability of each word in previous_ham being a ham email \n",
    "\n",
    "pspam = len(previous_spam) / (len(previous_spam) + len(previous_ham)) # From previous emails, figuring out the probability of spam or ham\n",
    "pham = len(previous_ham) / (len(previous_spam) + len(previous_ham)) # spam = 4/7, ham = 3/7 \n",
    "\n",
    "def classifier(email, spam_word_probs, ham_word_probs, pspam, pham): # put it all together for new_emails\n",
    "    email_words = email.split() # split up new emails \n",
    "    spam_prob = pspam # previously found spam probabilities\n",
    "    ham_prob = pham # previously found ham probabilities\n",
    "    \n",
    "    for word in email_words: # loop through all the words in the email         \n",
    "        if word in spam_word_probs: # If the word is in the spam_word dict, take that and multiply out the probabilitiy\n",
    "            spam_prob *= spam_word_probs[word] # multiplying this by the probability stored in the dict\n",
    "        else:\n",
    "            spam_prob *= 1 / (total_spam_words + len(spam_count))  # smoothing for unknown words using laplace smoothing, for words not found in data\n",
    "        if word in ham_word_probs: # same as above\n",
    "            ham_prob *= ham_word_probs[word]\n",
    "        else:\n",
    "            ham_prob *= 1 / (total_ham_words + len(ham_count)) # same smoothing as above\n",
    "    \n",
    "    if spam_prob > ham_prob: # I think the best way to do this is just to determine if p(spam) > p(ham), then it's probably spam \n",
    "        return \"Spam\"\n",
    "    else:\n",
    "        return \"Ham\"\n",
    "\n",
    "def save_new_info(classification, email): # updating the saved_ham, saved_spam lists with the classification that we found for the \"learning\"\n",
    "    if classification == \"Spam\":\n",
    "        saved_spam.append(email)\n",
    "    else:\n",
    "        saved_ham.append(email)\n",
    "\n",
    "for email in new_emails['spam']:\n",
    "    classification = classifier(email, spam_word_probs, ham_word_probs, pspam, pham) # for each new email, determine if Spam\n",
    "    print(str({email}) + \" is likely \" + str({classification})) # Print the output\n",
    "    save_new_info(classification, email) # save info in the new saved_spam or saved_ham list\n",
    "\n",
    "for email in new_emails['ham']: # same as above\n",
    "    classification = classifier(email, spam_word_probs, ham_word_probs, pspam, pham)\n",
    "    print(str({email}) + \" is likely \" + str({classification}))\n",
    "    save_new_info(classification, email)\n",
    "\n",
    "# The learning aspect is tricky - I just saved the outputs in saved_spam and saved_ham, so the \"previous_spam\" and \"previous_ham\" if this were\n",
    "# to be used more regularly and with different emails would be replaced with saved_spam and saved_ham. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated SPAM email list is now: ['renew your password', 'renew your vows', 'send us your password', 'review our website', 'send your password', 'send us your account']\n",
      "The updated HAM email list is now: ['benefits of our account', 'the importance of physical activity', 'Your activity report', 'benefits physical activity', 'the importance vows']\n"
     ]
    }
   ],
   "source": [
    "print(\"The updated SPAM email list is now: \" + str(saved_spam + previous_spam)) # printing the new saved_spam list with the emails we just determined\n",
    "print(\"The updated HAM email list is now: \" + str(saved_ham + previous_ham))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write you reflection in below cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was an interesting assignment that took me a lot of time to work through. I followed the core resources guide on the Naive Bayes Classifier and found a great video about this on the StatQuest YouTube channel, which was very helpful. For words that were not found in the training data, I struggled with what to do - the core resources example used (1-prob), where the prob was the probability that the email appeared in spam/ham, respectively. Ultimately, I decided to go with the formula 1/total spam + number of unique words in spam to ensure that any words not in the training data wouldn't result in a zero probability (since we are multiplying all of these probabilities, having a zero would throw off the result as 0). \n",
    "\n",
    "Once I could break down exactly what to calculate, it was not too bad, but figuring out the smoothing components and putting it all together was tough.\n",
    "\n",
    "I could not figure out a completely autonomous way to self-update the list, so I saved the new classified words in a new list instead. If the user wants to run new emails, one would just load the saved_spam and saved_ham as the previous emails to get this to work, but as mentioned, I was unable to automate this. \n",
    "\n",
    "I referenced a lot of my peer's code, mostly Henry's and Jakub, in coming up with a solution, and as mentioned, I used YouTube and the core resources to figure out what I was doing in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
